Purpose:
This evaluator measures the quality of the COURSE RETRIEVER only,
independent of downstream scoring, ranking, or UI logic.

Rationale:

- The retriever operates primarily on skill-based matching.
- As a result, it may retrieve courses that are highly relevant to the skill
  but misaligned with the user's contextual intent or domain.
- Such behavior is EXPECTED and acceptable at this stage.

Design Intent:

- Skill relevance and context alignment are evaluated as independent dimensions
  to expose different failure modes of retrieval.
- High skill relevance with low context alignment justifies the need for
  post-retrieval scoring and UI grouping in later stages of the pipeline.

Evaluation Philosophy:

- This evaluator is used for relative comparison and qualitative analysis,
  not as absolute ground truth.
- LLM-as-judge is applied consistently across all experiments to ensure
  comparability rather than exact correctness.

In short:
This evaluator answers: "Did the retriever pull courses related to the skill,
and what kinds of contextual mismatches naturally appear?"

ğŸ“ Comment â€” Criterion 1: Skill Relevance (Skill-only, Context Ignored)

Purpose: Measure how much the course actually teaches the skill, regardless of why the user wants it.

Score 3 â€” Strong Relevance

Definition:
The course is primarily designed to teach this skill. Most learning outcomes directly target it.

Example:
â€¢ Skill: Basic Programming
â€¢ Course: Introduction to Programming
â€¢ Reason: Core lectures, assignments, and assessments focus on programming fundamentals.

â¸»

Score 2 â€” Moderate Relevance

Definition:
The skill is an important component or tool, but not the sole focus of the course.

Example:
â€¢ Skill: Programming
â€¢ Course: Data Structures and Algorithms
â€¢ Reason: Programming is essential, but the course emphasizes algorithmic thinking rather than basic coding.

â¸»

Score 1 â€” Weak Relevance

Definition:
The skill appears only as a supporting topic, prerequisite, or minor part.

Example:
â€¢ Skill: Programming
â€¢ Course: Software Engineering
â€¢ Reason: Programming is assumed knowledge; the course focuses on process, teamwork, and design.

â¸»

Score 0 â€” Irrelevant

Definition:
The skill is not covered at all in the course content or learning outcomes.

Example:
â€¢ Skill: Programming
â€¢ Course: Digital Marketing
â€¢ Reason: No programming concepts are taught or required.

â¸»

ğŸ“ Comment â€” Criterion 2: Context Match (Intent & Domain Only)

Purpose: Measure how well the course fits what the user wants to do, regardless of how well it teaches the skill.

â¸»

Score 3 â€” Strong Alignment

Definition:
The course domain, depth, and application context directly match the userâ€™s intent.

Example:
â€¢ User intent: â€œà¸­à¸¢à¸²à¸à¹€à¸£à¸´à¹ˆà¸¡à¹€à¸‚à¸µà¸¢à¸™à¹‚à¸›à¸£à¹à¸à¸£à¸¡à¸à¸·à¹‰à¸™à¸à¸²à¸™â€
â€¢ Course: Introduction to Programming
â€¢ Reason: Matches beginner level, general purpose, and learning goal.

â¸»

Score 2 â€” Partial / Exploratory Alignment

Definition:
The course is in a related domain and useful for background or exploration, but not the most targeted option.

Example:
â€¢ User intent: â€œà¸­à¸¢à¸²à¸à¹€à¸£à¸´à¹ˆà¸¡à¹€à¸‚à¸µà¸¢à¸™à¹‚à¸›à¸£à¹à¸à¸£à¸¡à¸à¸·à¹‰à¸™à¸à¸²à¸™â€
â€¢ Course: Software Design and Architecture
â€¢ Reason: Same domain (software), but focuses on design concepts rather than hands-on beginner coding.

ğŸ‘‰ Useful as contextual knowledge, not a direct answer.

â¸»

Score 1 â€” Context Mismatch

Definition:
The course uses similar skills but applies them in a different domain or purpose than the user intends.

Example:
â€¢ User intent: General coding skills
â€¢ Course: Programming for Business Analytics
â€¢ Reason: Programming is applied specifically to business/data problems, not general software development.

ğŸ‘‰ Skill overlap â‰  intent alignment.

â¸»

Score 0 â€” Irrelevant

Definition:
The course domain and application context are clearly unrelated to the userâ€™s goal.

Example:
â€¢ User intent: Learning programming
â€¢ Course: Introduction to Psychology
â€¢ Reason: No domain or intent overlap.

â¸»

One-liner you can say if grilled in Q&A ğŸ¯

â€œSkill Relevance measures what the course teaches, while Context Match measures why and for whom it is useful.â€

That sentence alone can save you 10 minutes of rambling.

## ğŸ“Š Summary of Retrieval Evaluation Metrics

These metrics evaluate retriever behavior, not absolute correctness.
They are used for relative comparison, diagnosis, and explanation.

â¸»

ğŸ”¹ averageSkillRelevance

What it measures
How well the retriever finds courses that actually cover the requested skill.

Interpretation
â€¢ High â†’ Retriever is good at subject/topic matching
â€¢ Low â†’ Retriever fails to surface courses teaching the skill

Why it exists
â€¢ Evaluates subject coverage independent of user intent

â¸»

ğŸ”¹ skillRelevanceDistribution

What it measures
The distribution of skill relevance scores (0â€“3) across retrieved courses.

Interpretation
â€¢ Many 3s â†’ Strong topical retrieval
â€¢ Many 0s/1s â†’ Noisy or weak retrieval

Why it exists
â€¢ Shows whether retrieval quality is consistently good or scattered

â¸»

ğŸ”¹ averageContextAlignment

What it measures
How well retrieved courses align with the userâ€™s intent, domain, and application context.

Interpretation
â€¢ High â†’ Retriever understands what the user is trying to achieve
â€¢ Low â†’ Retriever finds relevant skills in the wrong domain

Why it exists
â€¢ Evaluates user understanding, not just keyword matching

â¸»

ğŸ”¹ contextAlignmentDistribution

What it measures
The spread of context alignment scores (0â€“3).

Interpretation
â€¢ Many 3s â†’ Context-aware retrieval
â€¢ Many 1s â†’ Skill matches but domain mismatch

Why it exists
â€¢ Helps explain why results feel irrelevant to users

â¸»

ğŸ”¹ alignmentGap
alignmentGap = averageSkillRelevance - averageContextAlignment
What it measures
The gap between subject relevance and context understanding.

Interpretation
â€¢ â‰ˆ 0 â†’ Balanced retrieval
â€¢ 0 â†’ Skill-first retrieval (expected)
â€¢ < 0 â†’ Context looks right, skill coverage is weak

Why it exists
â€¢ Diagnoses whether post-retrieval scoring and UI grouping are needed

â¸»

ğŸ”¹ contextMismatchRate

What it measures
Percentage of courses with high skill relevance but low context alignment.

Interpretation
â€¢ High â†’ Retriever finds correct skills in wrong domains
â€¢ Low â†’ Retriever already context-aware

Why it exists
â€¢ Justifies:
â€¢ post-retrieval scoring
â€¢ relevance grouping
â€¢ contrast-based UI (e.g., Sankey)

â¸»

ğŸ”¹ contextMismatchCourses

What it contains
List of courses that:
â€¢ Teach the skill well
â€¢ But do not match the userâ€™s intent/domain

Why it exists
â€¢ For qualitative inspection
â€¢ For visualization and explanation in demos

â¸»

ğŸ§  Design Philosophy (Reminder for Future You)
â€¢ Scores are approximate relevance levels, not decomposed metrics
â€¢ LLM is used as a consistent judge, not a ground-truth oracle
â€¢ Metrics explain system behavior, not human learning outcomes
