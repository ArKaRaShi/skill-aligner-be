NODE_ENV=development
APP_DEBUG=true
PORT=3001
# DATABASE_URL=postgresql://user:password123@host.docker.internal:5433/appdb?schema=public

# Embedding configuration
SEMANTICS_API_BASE_URL='http://localhost:8000/api/v1/semantics'
EMBEDDING_MODEL='text-embedding-3-small'  # Options: e5-base, text-embedding-3-small
# EMBEDDING_MODEL='e5-base'  # Options: e5-base, text-embedding-3-small

# DATABASE_URL="postgresql://<user>:<password>@<host>:6543/postgres?pgbouncer=true"
# DIRECT_URL="postgresql://<user>:<password>@<host>:5432/postgres"

# DATABASE_URL=postgresql://user:password123@localhost:5433/appdb?schema=public
# DIRECT_URL=postgresql://user:password123@localhost:5433/appdb

DATABASE_URL=postgresql://testuser:testpassword123@localhost:5434/testdb?schema=public
DIRECT_URL=postgresql://testuser:testpassword123@localhost:5434/testdb

OPENAI_API_KEY=

# OpenRouter configuration
# OPENROUTER_API_KEY=xxx
OPENROUTER_API_KEY=
OPENROUTER_BASE_URL='https://openrouter.ai/api/v1'

# Zai configuration
ZAI_BASE_URL='https://api.z.ai/api/paas/v4/'
ZAI_API_KEY=

# LLM configuration
DEFAULT_LLM_PROVIDER='openrouter'

# Mock service toggles
USE_MOCK_QUESTION_CLASSIFIER_SERVICE=false
USE_MOCK_QUERY_PROFILE_BUILDER_SERVICE=false
USE_MOCK_SKILL_EXPANDER_SERVICE=false

# LLM Models (format: "provider/model-name" or just "model-name" to use default provider)
# In main usecase pipeline
QUESTION_CLASSIFIER_LLM_MODEL='gpt-4.1-nano'
SKILL_EXPANDER_LLM_MODEL='gpt-4.1-mini'
COURSE_RELEVANCE_FILTER_LLM_MODEL='gpt-4.1-nano'
ANSWER_SYNTHESIS_LLM_MODEL='grok-4.1-fast'

# Unused LLM Models / Deprecated
QUERY_PROFILE_BUILDER_LLM_MODEL='gpt-4.1-nano'
FILTER_LO_LLM_MODEL='gpt-4.1-nano'

# Cache toggles
USE_QUESTION_CLASSIFIER_CACHE=false
USE_SKILL_EXPANDER_CACHE=false
